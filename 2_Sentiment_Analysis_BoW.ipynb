{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amos-fernandes/ufg-ext-LLM-Langchain-LlamaIndex/blob/main/2_Sentiment_Analysis_BoW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Análise de Sentimento\n",
        "\n",
        "Análise de sentimento é o processo de automatizar a rotulação dos dados de texto de acordo com o sentimento da sentença (positivo, neutro e negativo). Análise de sentimento permite que as empresas possam analisar os dados em escala, detectando insights e automatizando processos.\n",
        "\n",
        "Neste notebook, por exemplo, iremos analisar milhares de reviews do Play Store de aplicativos de compras. Ao invés de analisar estes dados manualmente, podemos usar análise de sentimento para entender automaticamente o que as pessoas estão pensando sobre os aplicativos, permitindo tomar decisões de negócio baseadas em dados.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1Qa0W_cwDOzb62xDUI8s0uleg0qJjWgN4\" width=\"50%\" height=\"50%\">"
      ],
      "metadata": {
        "id": "62zPVh0M__le"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Análise de sentimento com BOW\n",
        "Para construir um modelo de análise de sentimento utilizando a abordagem de vetorização BOW, precisamos de um conjunto de dados rotulados. Os algoritmos de aprendizado de máquina são bons com números e, por isso, temos que extrair ou converter os textos em números com a perda do mínimo de informações. Uma forma de fazer isso é utilizar o Bag-Of-Words (BOW) que atribui um número para cada palavra. Para vetorizar as palavras, podemos utilizar:\n",
        "\n",
        "* **CountVectorizer**: conta o número de palavras no documento, ou seja, converte uma coleção de documentos de texto em uma matriz com a contagem de ocorrências de cada palavra no documento.\n",
        "* **TfidfVectorizer**: TF-IDF (Term-Frequency-Inverse-Document Frequency) reduz o peso das palavras que ocorrem na maioria dos documentos e dá maior importância para palavras que aparecem em um subconjunto de documentos. Ou seja, o TF-IDF penaliza as palavras mais frequentes e dá maior importância para palavras raras em determinados documentos."
      ],
      "metadata": {
        "id": "feqa4wwTl0A2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configurando o ambiente de execução"
      ],
      "metadata": {
        "id": "bwWn9qUD0pEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "pd.set_option('display.max_colwidth',1000)"
      ],
      "metadata": {
        "id": "8uaEXidxSWAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explorando os dados de treinamento\n",
        "Iremos baixar o arquivo `reviews_complete.csv` que contém quase **12k reviews** de aplicativos do PlayStore."
      ],
      "metadata": {
        "id": "JUUDYHVoK5af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1wf-3sf_zkSi1VMhVSMcWfyX7wWVEY2R5"
      ],
      "metadata": {
        "id": "2t36bWtVTRYs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd7bec4d-8ebe-4ddf-890a-2b4954f89a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wf-3sf_zkSi1VMhVSMcWfyX7wWVEY2R5\n",
            "To: /content/reviews.csv\n",
            "\r  0% 0.00/6.78M [00:00<?, ?B/s]\r100% 6.78M/6.78M [00:00<00:00, 91.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"reviews.csv\")"
      ],
      "metadata": {
        "id": "KMw43xmzK-n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "qmDh_sCnTl-u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f98bad4-98c3-4f8c-99fa-662d2c7f90e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(12000, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparando o ambiente\n",
        "\n",
        "Iremos utilizar a bilioteca nltk para facilitar o processamento dos textos. NLTK, que significa Natural Language Toolkit, é uma biblioteca disponível na linguagem Python com o intuito de realizar tarefas de NLP. É uma das principais bibliotecas de estudo para quem está ingressando nessa área, pois nela se encontram vários datasets e alguns algoritmos essenciais para análise e pré-processamento de textos.\n",
        "\n",
        "Aqui utilizamos quatro pacotes do nltk:\n",
        "\n",
        "* **stopwords**: pacote com uma lista de stopwords, incluíndo a língua portuguesa.\n",
        "* **punkt**: pacote para tokenização que divide um texto em uma lista de sentenças utilizando um algoritmo não supervisionado.\n",
        "* **wordnet**: banco de dados léxicos. Aqui utilizamos na operação de Lemmatization.\n",
        "* **omw-1.4**: Open Multilingual Wordnet. Aqui utilizamos na operação de Lemmatization."
      ],
      "metadata": {
        "id": "vuqihCscVLm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "6fadhOYTqwEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download pt_core_news_sm"
      ],
      "metadata": {
        "id": "SKMKP21JIbud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('pt_core_news_sm')"
      ],
      "metadata": {
        "id": "HjHg6bqiI0GV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pré-processamento dos dados\n",
        "\n",
        "O pré-processamento dos dados textuais de treinamento envolvem as seguintes tarefas:\n",
        "\n",
        "* **Normalização**: remoção de dígitos, pontuação, lower case, etc..\n",
        "* **Tokenização**: segmenta o texto em palavras\n",
        "* **Remoção de stopwords**: são um conjunto de palavras bastante utilizadas em uma linguagem e, geralmente, não tem importância para entender o significado principal do texto. Exemplo: \"o\", \"da\", \"é\" e \"para\".\n",
        "* **Lemmatization**:  processo de deflexionar uma palavra para determinar o seu lema. Por exemplo, as palavras gato, gata, gatos, gatas são todas formas do mesmo lema: gato."
      ],
      "metadata": {
        "id": "C98Z4leEd_f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(u'O tema do workshop até que é bom. O problema é o professor!')\n",
        "doc.text.split()"
      ],
      "metadata": {
        "id": "c2RwXptxzdJM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ce9586-6bb1-48db-9e82-98c5ddce5aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['O',\n",
              " 'tema',\n",
              " 'do',\n",
              " 'workshop',\n",
              " 'até',\n",
              " 'que',\n",
              " 'é',\n",
              " 'bom.',\n",
              " 'O',\n",
              " 'problema',\n",
              " 'é',\n",
              " 'o',\n",
              " 'professor!']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "def pre_processing(df, stop_words):\n",
        "    # Lower case\n",
        "    df['content'] = df['content'].apply(lambda x: ' '.join(x.lower() for x in x.split()))\n",
        "    # remoção de dígitos ou números\n",
        "    df['content'] = df['content'].str.replace('\\d+', '')\n",
        "    # Remoção de stop words\n",
        "    df['content'] = df['content'].apply(lambda x: ' '.join(x for x in x.split() if x not in stop_words))\n",
        "    # Lemmatization\n",
        "    df['content'] = df['content'].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)]))\n",
        "    return df\n",
        "stop_words = stopwords.words('portuguese')\n",
        "df = pre_processing(df, stop_words)"
      ],
      "metadata": {
        "id": "btIw6XBatFso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pré-processamento dos dados utilizando `TfidfVectorizer` para construir o Bag of Word. Parâmetros:\n",
        "\n",
        "* **ngram_range**: os limites inferiores e superiores da faixa de valores para os n-grams. Por exemplo, o ngram_range igual a (1,2) significa que podem ser gerados unigramas e bigramas.\n",
        "* **sublinear_tf**: aplica um scaling no tf, ou seja, substitui o tf por 1 + log(tf)."
      ],
      "metadata": {
        "id": "_V0Dhbjwgrtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['content'].head()"
      ],
      "metadata": {
        "id": "cv7b2IuY-eqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5550f4ad-f6e5-4b9a-cc6c-0baf00896175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                                                                                                                horrível . informação enganador , principalmente medida . dois produto mesmo suposto medida peito ombro , dois completamente diferente . claramente 2 3 tamanho acima outro . além caro qualidade apresentar material . primeiro último compra .\n",
              "1    primeiro experiência plataformo . bom preço , variedade . entregar internacional prazo . entregar nacional péssima ! ! ! ! ainda recebir produto , atrasar com si nenhum contato . apenas rastreamento dizer q entregar .... qdo ? ? ? decepcionar . continuar nenhum informação pedido super atrasar ! ! ! . \" suporte \" responder nenhum contato . absurdo\n",
              "2                     gostar nada de esse plataforma compra , fiz pedir devido problema sério endereço cancelar pedir , porém , cancelar endereço lá simplesmente existir tento contato suporte técnico consigo . horrível ! ! primeiro compra nunca mais , cliente shopee lá todo assistência preciso . ficar alertar pra gente , cuidado comprar em esse site .\n",
              "3                                                                                      péssimo . incrível poder 5 estrela , completo descaso responder mensagem absurda . fiz compra 6 item , recebir 5 ( o compra quase 1 mês , todo hoje constar \" em processamento \" ) avaliar recebir reclamar faltar . mar mensagem nada . tudo pagar , então prejuízo meu .\n",
              "4                                                                                                                                      aplicativo bom mexer . 3 primeiro compra atraso cerca 2 semana postar pedir ser dizer descrição disponível estoque . entregar atraso último pedir fiz 1 mês hoje postar , aplicativo opção cancelar compra pedir estorno .\n",
              "Name: content, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "cv = TfidfVectorizer(ngram_range=(1,2),\n",
        "                     sublinear_tf=True)\n",
        "text_counts = cv.fit_transform(df['content'])"
      ],
      "metadata": {
        "id": "XMSlKvp-qFzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_counts[0]"
      ],
      "metadata": {
        "id": "e_y1XmYt-49T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dee4927d-b268-42c9-a3c6-9786cdfd7267"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x129590 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 51 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os dados devem ser divididos em conjunto de treinamento e de teste. O código abaixo aloca 75% da base de dados para treinamento e 25% para teste."
      ],
      "metadata": {
        "id": "qRlDc9gUhTE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dividindo os dados em treinamento e teste\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(text_counts, df['sentiment'], test_size=0.25, random_state=5)"
      ],
      "metadata": {
        "id": "GLW9uxupp6cM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento do modelo\n",
        "O modelo será treinado utilizando os dados de treinamento `X_train` e `Y_train` utilizando Regressão Logística. Este modelo pode ser substituído por vários outros, tais como SVM, Random Forest e Decision Trees. Aqui a Regressão Logística é utilizada por ser computacionalmente eficiente no treinamento e predição."
      ],
      "metadata": {
        "id": "5Qa423MkhfzN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Treinando o modelo\n",
        "# from sklearn import svm\n",
        "# SVM = svm.SVC(kernel='linear')\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "LR = LogisticRegression(max_iter=300)\n",
        "LR.fit(X_train, Y_train)"
      ],
      "metadata": {
        "id": "MR13Ok68qUfl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "e5db9d07-7d3a-48b8-ab18-9425e9aea495"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=300)"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=300)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=300)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na célula abaixo calculamos o score do modelo:"
      ],
      "metadata": {
        "id": "PzY_XNJxiEis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "predicted = LR.predict(X_test)\n",
        "accuracy_score = metrics.accuracy_score(predicted, Y_test)\n",
        "print(\"Accuracy Score: \",accuracy_score)"
      ],
      "metadata": {
        "id": "j4RLKOxBqWaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec9f9fd2-16a9-4606-eb9f-aac56801345d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score:  0.6426666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predição com dados reais do Play Store\n",
        "\n",
        "Abaixo utilizamos alguns textos extraídos de reviews do Play Store para verificar o quão o modelo está bom em textos reais. Utilizamos 3 sentenças (1 positiva, 1 neutra e 1 negativa) para realizarmos a avaliação do modelo em algumas sentenças escolhidas."
      ],
      "metadata": {
        "id": "O0OXl1ZriH73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['negative', 'neutral', 'positive']"
      ],
      "metadata": {
        "id": "AXEXeTpt_L_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative_review = \"\"\"Péssimo para quem quer vender. Além das altas taxas cobradas, demora para atualizar os encaminhamentos do produto\"\"\"\n",
        "neutral_review = \"\"\"Muito prático e fácil de usar, mas para conseguir ajuda é terrível!\"\"\"\n",
        "positive_review = \"\"\"Eu adoro o app enjoei, em todos os sentidos, acho bem intuitivo, fácil de procurar os ítens que você está querendo comprar e a interface é linda, bem clean e agradável aos olhos.\"\"\"\n",
        "reviews = [negative_review, neutral_review, positive_review]"
      ],
      "metadata": {
        "id": "EvkA1HpUsPvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for review in reviews:\n",
        "  review_vector = cv.transform([review]) # vetorização\n",
        "  prediction = LR.predict(review_vector)[0]\n",
        "  print(f'Review text: {review}')\n",
        "  print(f'Sentiment  : {class_names[prediction]}')"
      ],
      "metadata": {
        "id": "3UEcQR3oZiHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fab9d986-ae4e-430a-f2dc-377fb158880c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review text: Péssimo para quem quer vender. Além das altas taxas cobradas, demora para atualizar os encaminhamentos do produto\n",
            "Sentiment  : negative\n",
            "Review text: Muito prático e fácil de usar, mas para conseguir ajuda é terrível!\n",
            "Sentiment  : positive\n",
            "Review text: Eu adoro o app enjoei, em todos os sentidos, acho bem intuitivo, fácil de procurar os ítens que você está querendo comprar e a interface é linda, bem clean e agradável aos olhos.\n",
            "Sentiment  : positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Referências\n",
        "\n",
        "* https://www.kaggle.com/code/divsinha/sentiment-analysis-countvectorizer-tf-idf/notebook\n",
        "* https://towardsdatascience.com/a-beginners-guide-to-sentiment-analysis-in-python-95e354ea84f6\n",
        "* https://leportella.com/pt-br/npl-com-spacy/\n",
        "* https://medium.com/@van3ssabandeira/o-famoso-spacy-90afb683b6fe\n",
        "* https://www.alura.com.br/artigos/lemmatization-vs-stemming-quando-usar-cada-uma"
      ],
      "metadata": {
        "id": "HR69zuwgcneo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rEKHQnRt6-hQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}